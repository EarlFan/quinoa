/*!
  \page      inciter_design Inciter software design

This page discusses the high-level software design and some implementation
aspects of @ref inciter_main. The discussion follows as execution happens in
time, from program start to finish.

@section startup 1. Startup and migration of read-only global-scope data

## Startup

As all other executables in Quinoa, Inciter uses the Charm++ runtime system.
Runtime execution starts in the Charm++ `mainmodule` constructor, defined in
Main/Inciter.C as `Main`. Starting down the constructor's initializer list, the
command line is parsed, followed by creating a driver, inciter::InciterDriver,
which parses the input file.

## Command line and input file parsing, global-scope data

After the main chare constructor has finished, the runtime system initializes
global scope data and migrates it to all other processing elements (PEs), which
from that point is considered read-only.

@warning Note that this global-scope is _assumed_ read-only after migration but
the compiler has now way of enforcing this, thus modifying it after migration
will not generate a compile-time error.

This global-scope data are defined at the beginning of Main/Inciter.C in the
same order as they appear in the companion Charm++ interface file
Main/inciter.ci. This is the order in which these data are initialized and
migrated. Global scope data is limited to such read-only data.  It stores data
initialized during parsing the command line and the input (control) file. The
command line is parsed by inciter::CmdLineParser's constructor, called during
the main chare's constructor, while the input file is parsed by
inciter::InputDeckParser's constructor, called during InciterDriver's
constructor.

Global_scope data is always prefixed by `g_`. Inciter's global-scope data
contains two input decks: the defaults (inciter::g_inputdeck_defaults) and
inciter::g_inputdeck which contains all data parsed during command line and
input file parsing.

## Global scope data for runtime polymorphism

Two other global scope vectors are `inciter::g_cgpde` and `inciter::g_dgpde`.
These two vectors hold base classes for partial differential equations (PDE)
that can be specialized to different types of derived PDEs that use continuous
Galerkin finite element (FE) discretizations (inciter::CGPDE) and discontinuous
Galerkin FE discretizations (inciter::DGPDE). These two vectors are global
scope because they hold pointers to derived classes that enable runtime
polymorphism. Runtime polymorphism enables code reuse and helps client code
uniform and modular. Such polymorphic use of pointers (`std::unique_ptr`) are
one of the rare uses of pointers throughout the code.

@note While Charm++ allows migratable chares holding pointers and even supports
runtime polymorphism among chare arrays, we strongly discourage the use of
pointers and reference semantics, because it complicates writing safe code for
such migratable objects. For more details on reference and value semantics in
C++, see https://isocpp.org/wiki/faq/value-vs-ref-semantics. For more details
on runtime polymorphism without client-side inheritance, the style in which the
"base" classes inciter::CGPDE and inciter::DGPDE are written, that leads to
cleaner and safer client code, see Sean Parent's talk on [concept-based
polymorphism](http://sean-parent.stlab.cc/papers-and-presentations/#value-semantics-and-concept-based-polymorphism).
For more details on concept-based runtime polymorphism with migratable Charm++
chare arrays using value semantics, see Inciter/Scheme.h and [this
talk](http://charm.cs.illinois.edu/charmWorkshop/slides/CharmWorkshop2018_bakosi.pdf).

## Create inciter's driver, single Charm++ chare _Transporter_

Up to this point execution is serial, since there is only a single main Charm++
chare. `InciterDriver`'s constructor then fires up a single Charm++ chare
instance of `Transporter`, called from Main::execute() after the runtime system
has finished migrating all global-scope data. Note that InciterDriver is _not_
a Charm++ chare, only an ordinary C++ class. `Transporter`, defined in
Inciter/Transporter.C, is the main driver class of Inciter that is a Charm++
chare from which all execution happens, e.g., via broadcasts, and to which all
execution ends up in, leading to `Transporter::finish()`, which eventually
calls back to `Main::finalize()`, calling `CkExit()`, signaling the runtime
system to exit.

@section classes 2. Important classes

Here are the important classes that interoperate within inciter:

 - `Transporter` (single chare, driver)
 - `Solver` (chare _group_, linear system solver)
 - `Partitioner` (chare _group_, mesh partitioner)
 - `Refiner` (chare _array_, mesh refiner)
 - `Sorter` (chare _array_, performs mesh reordering)
 - `Discretization` (chare _array_, generic PDE solver base class)
   - `DiagCG` (chare _array_, PDE solver child class, specialized to continuous
     Galerkin finite element discretization scheme with a lumped-mass left-hand
     side and flux-corrected transport combined with Lax-Wendroff time stepping),
   - `MatCG` (chare _array_, PDE solver child class, specialized to continuous
     Galerkin finite element discretization in space with a consistent-mass
     left-hand side and a linear solver with flux-corrected transport combined
     with Lax-Wendroff time stepping),
   - `DG` (chare _array_, PDE solver child class, specialized to discontinuous
     Galerkin finite element discretization scheme with a diagonal left-hand
     side with Runge-Kutta time stepping),
   - ... (more hydro schemes to be added in the future)
 - `DistFCT` (chare _array_, performs distributed-memory flux-corrected
   transport, if used by a specific discretization scheme)

_Chare_ above means a single Charm++ chare. There is a single instance of this
class. By design, `Transporter` is a single chare and is used as a driver that
creates objects, it is as a target of global parallel reductions, and thus
global synchronization points. `Transporter` also does most of the printouts to
screen, collects statistics, and the end-point of an execution in
`Transporter::finish()`.

_Group_ above means a Charm++ chare group. A group is a processor-aware chare
collection, which means that there is guaranteed to be a single instance of a
group per PE which does _not_ migrate. Examples in inciter are `Solver` and
`Partitioner` both of which are groups because they call MPI-only libraries,
which require calls from PEs.

@note Charm++ chare groups `Solver` and `Partitioner` are currently in the
process of being converted to Charm++ `nodegroup`s, which are similar to groups
in that they are processor-aware but instead of one per PE, nodegroups are
guaranteed to be one per logical (compute) node, where a _compute node_ is
understood to be a logical node which may or may not be equivalent to a
physical compute node (i.e., a shared-memory node), depending on how a given
execution environment is configured in Charm++'s symmetric multi-processing
mode (SMP) mode. (For example, a run in SMP mode can be configured with a
logical node corresponding to a shared-memory compute node or to a socket
within a compute node). Similar to groups, nodegroups do not migrate but there
is a significant difference in their parallel computing semantics in how the
runtime system invokes their member functions compared to groups. Note that
there is no difference between gorups and nodegroups if Charm++ is built in
non-SMP mode, which is currently the default. For more details see the Charm++
manual.

_Array_ above means a Charm++ chare array, whose elements can migrate (if
enabled) and thus they actively participate in automatic load balancing. With
nonzero overdecomposition, configured with the `-u` command line argument,
there may be more array elements (workers) than the number of available PEs.
Arrays do the bulk of the heavy lifting in a calculation, i.e., computing
right-hand sides for PDE operators, and hold the bulk of unknown/solution
arrays. The degree of overdecomposition can be specified by the `-u` command
line argument. This argument accepts a real value between 0.0 and 1.0,
inclusive. 0.0 means no overdecomposition, which corresponds to partitioning
the mesh into a number of pieces equalling the number of PEs available. (`-u
0.0` is how MPI codes are traditionally used.) Nonzero overdecomposition yields
larger number of mesh partitions than the available PEs. The extreme of `1.0`
represents the largest degree of overdecomposition, which also results in the
smallest work units. For a discussion on the effects of overdecomposition, see
the page on @ref inciter_performance.

_Bound arrays_: `Discretization` and its specialized children, `DiagCG`, `DG`,
etc., `Refiner`, and, if used/created, `DistFCT` are _bound_ arrays. This means
that the runtime system migrates its corresponding array elements together.
Bound arrays facilitate modularization among workers that migrate.

@section setup 3. Setup

...

*/
